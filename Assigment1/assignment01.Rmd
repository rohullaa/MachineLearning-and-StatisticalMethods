---
title: "STK2100 - Machine learning and statistical methods"
subtitle: "Mandatory assignment 1"
author: "Rohullah Akbari"
date: "2/16/2021"
header-includes:
  - \usepackage{xcolor}
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Problem 1

### a)
We use the following code to make the data available:
```{r}
library(MASS)
datadir = "http://www.uio.no/studier/emner/matnat/math/STK2100/data/" 
nuclear = read.table(paste(datadir,"nuclear.dat",sep=""),header=T)
n = nrow( nuclear )
```
In order to be able to access the data directly by the variables, we use the attach() function. Our interest will be in the "cost" variable and since this variable is always positive, we will model this variable at the log-scale.
```{r}
attach(nuclear)
nuclear["cost"] = log(cost)
names(nuclear)[names(nuclear) == "cost"] <- "logcost"
attach(nuclear)
head(nuclear)
```
Here we can see that the "pr","ne","ct","bw" and "pt" variables are categorical variables since they consist of only 0's and 1's. Therefore, we plot without them. The matrix-plot of the data:
```{r}
nuclear.plot = subset(nuclear,select = -c(pr,ne,ct,bw,pt))
pairs(nuclear.plot)
```

We use boxplot to plot the categorical variables:
```{r}
boxplot(logcost ~ pr)
boxplot(logcost ~ ne)
boxplot(logcost ~ ct)
boxplot(logcost ~ bw)
boxplot(logcost ~ pt)
```

### b)

We will first look at a model:
$$Y_i =\beta_0 +\beta_1 x_{i,1} +···+\beta_p x_{i,p} +\epsilon_i$$
where $Y_i$ is cost at log-scale for observation i.

The standard assumptions about the noise terms $\epsilon_i$ is that 
$$ \epsilon_1 ... \epsilon_n \sim N(0,\sigma ^2)$$ 
In other words that the noise terms are normal distributed. The noise terms are independent of each other, the expectation value of them is 0 and the variance of them is $\sigma^2$, where $\sigma^2$ is a positive constant value for all replications. 

Fitting this model:
```{r}
fit1 = lm(logcost ~.,data=nuclear)
summary(fit1)
```
Here we can see that the p-value of the model is very small ($5.717 \cdot 10^{-07}$), meaning that some of the variables are useful. By looking at the p-value for the individual variables, we can see that variables such as "t1" and "bw" have high p-value. We can conclude that this are not very useful and could remove them from the model. 

### c)
In general, we do a hypothesis-testing to determine if a variable should be thrown away or not. In this case, we do the following test:
$$H_0: \beta_j = 0 $$
From STK1110, we know that the smaller p-value the more evidence there is in the sample data against the $H_0$ and vice versa. Therefore, we start by removing the variable with the highest p-value ("t1").
```{r}
fit2 = lm(logcost ~.-t1,data=nuclear)
summary(fit2)
```
The R-squared factor has now increased from 0.7985 to 0.8072, meaning that the new model has a little bit better linear relationship with explanatory variables and logcost. In addition to that, the p-value of some variables has also decreased. It makes sense since the variables can behave differently in different models. We can see from matrix-plot above that "date" and "t1" have a strong correlation between them, and therefore by removing the "t1" will make "data" change its p-value drastic. 

This method also applies to the general interpretation of a new model. By removing the variable with the highest p-value, will affect the p-value of the other variables who has a strong correlation the removed variable.

### d)
The procedure for continuing to remove explanatory variables, until all p-values are less than 0.05, is to remove a variable at the time and check if the given condition is satisfied. Run it until it is satisfied.

In this task, the procedure is been done manually by using the summary command to check the p-value and remove the variable with highest. By continuing this, we remove first "bw". In the next round we remove "pr", and so on. The final model is: 
```{r}
fit3 = lm(logcost ~.-t1 -bw -pr-t2 -cum.n -ct,data=nuclear)
summary(fit3)
plot(fit3)
```

The plots shows the linearity of the new model. There are some points on the plot that are non-linearity, especially the one at the lower and the two at the upper quantile (points nr. 7,10,19). But other than that, the new model follow normal distribution quite good. It is although difficult to say more about the non-linearity because the limited data we have.

### e)
The average quadratic error is giving by:

$$ MSE = \frac{1}{n} \sum_{i =n}^n (y_i - \hat f(x_i) ) =  \frac{1}{n} \sum_{i =n}^n (y_i - \hat y_i )  $$
Here we have to predict the response (yhat), and then calculate the MSE.
```{r}
yHat = predict(fit3,newdata = nuclear)
yi = logcost
MSE = 1/n * sum((yi - yHat)^2)
MSE
```

MSE tells us the error between the predicted value and the actual value. In simple words, we can say the lower value of MSE the better model, and if the value of MSE is equal to 0 then the model is perfect since there is no error. In this case, we got 0.026 which tells that the model is quite "good". However this procedure does not tell us exactly how good the new model is and we do not get an answer for what percentage of the nuclear-data fits with the model. 

### f)
We start by writing the Gaussian and then the likelihood function:

$$ f(y;\theta) = \frac{1}{\sqrt {2 \pi \sigma^2}} e^{- \frac{(y_i - \theta)^2}{2 \sigma ^2} } $$
$$L(\theta) = \prod_{i=1}^n f(x;\theta)$$
In order to make the equation easier to evaluate, we take the natural logarithm of it:
$$ \log L(\theta) = \log (\prod_{i=1}^n f(x;\theta))  $$
$$ = \sum_{i = 1} ^n -\log (\sqrt {2 \pi \sigma^2}) - \frac{(y_i - \theta)^2}{2 \sigma ^2} = \sum_{i = 1} ^n const - \frac{1}{2} \log \sigma^2 - \frac{(y_i - \theta)^2}{2 \sigma ^2}   $$
$$= const -\frac{n}{2} \log \sigma^2 - n \frac{(y_i - \theta)^2}{2 \sigma ^2} $$
By declaring an estimate for $\sigma^2$ as $\hat \sigma^2 = (y_i - \theta)^2$, we get:
$$ \log L(\hat\theta) = const -\frac{n}{2} \log (\hat \sigma^2) - \frac{n}{2} $$
Here $n$ is the amount of variables we have in the model, and therefore we can add the $-n/2$ into the constant-term. We get:
$$ \log L(\hat \theta) = const -\frac{n}{2} \log (\hat \sigma^2)$$
By inserting this expression into the equations for AIC and BIC, we get
$$AIC = -2\log(L(\hat \theta)) + 2(q+2) = const + n\log(\hat \sigma^2)+2(q+2)  $$
$$BIC = -2\log(L(\hat \theta)) + \log(n)(q+2) = const + n\log(\hat \sigma^2)+ \log(n)(q+2) $$

### g)

We start by AIC:
```{r}
fullModel = lm(logcost ~.,data = nuclear)
AICModel = stepAIC(fullModel,direction = "backward", k =2)
```

BIC:
```{r}
BICModel = stepAIC(fullModel,direction = "backward", k =log(n))
```

We end up with following models:
```{r}
summary(AICModel)
```

```{r}
summary(BICModel)
```

Both methods start with models with all explanatory variables, but decrease with one variable at a time until it has reached the best solution. The variables that we are left with after both methods has run: 

* AIC: date + t2 + cap + pr + ne + ct + cum.n + pt
* BIC: date + cap + ne + ct + pt 

### h)
The order between AIC and BIC is the same when stepAIC runs the selection. This is due to the penalty term, which is defined as a given limit multiplied by a specific constant. In other words, the penalty term is the same for both AIC and BIC within the same dimension. In addition, both models have the same log-likelihood value, and since order is determined by log-likelihood, it makes sense that both models have the same order. 

### i) 
From task 1d) and 1g) we are left with models containing the following variables:

* Manually: date,cap,ne,pt
* AIC: date + t2 + cap + pr + ne + ct + cum.n + pt
* BIC: date + cap + ne + ct + pt 

The order the other variables were removed:

* Manually: -t1 -bw -pr -t2 -cum.n -ct
* AIC: -t1 -bw
* BIC: -t1 -bw -pr -t2 -cum.n

We can see that the "manually selection" has the least amount of variables, while the AIC and BIC has more. That is the case because of the manually selection occur by removing variables one by one and independently of the model as whole, while the AIC and BIC do the opposite. They select and evaluate the model as a whole. This means that even if a variable in AIC or BIC has a non-significant p-value, it is not necessarily removed and thus can lead to the model being improved as a whole. Another interesting finding is the variables and the order of those that were removed from the various models. We can see that the same variables were removed, and they were removed in the same order. 

### j)
We assume that $Z \sim N(\mu,\sigma^2)$ and will show that $E(e^Z) = exp(\mu + 0.5 \sigma^2)$. To do that, we need little calculation. Just to make it easier, we will use moment generating functions. From STK1100, we have that a moment generating function for a stochastic variable is giving by:
$$ M_Z (t) = E(e^{tZ}) $$
From formula collection in STK1110/STK1100, we have the moment generating function for a normal distribution:
$$M_Y (t) = e^{\mu t} e^{\sigma ^2t^2 /2} $$
The expectation value is obtained when $t=1$ in the $M_Z(t)$ function above:
$$ E(e^{Z}) = M_Z (1) = e^{\mu} e^{\sigma ^2 /2}  = e^{\mu + \sigma^2 /2}$$
Which is the same expression as the one we wanted to show. We can use this to argue for $\hat \eta_2$ since it can be obtained directly from the expression above. By inserting $\theta =  \mu$, we get the estimate:
$$ \hat \eta_2 = e^{\hat \theta + 0.5 \sigma^2} $$
In order to obtain $\hat \eta_1$ from the expression $E(e^Z)$ we must say that $E(e^{Z})=e^{E(Z)}= e^\theta$ and that is not correct. Therefore, we can conclude with the $\hat\eta_2$ is correct.

### k)
The code goes as follow:

* First: defining the dataframe d.new
* Calculating the values for the $\theta$s for all three models
* Finding sigma-values for the three models
* Finally: calculate the $\eta$s by the expression given above.

```{r}
d.new = data.frame(date=70.0,t1=13,t2=50,cap=800,pr=1,ne=0,ct=0,bw=1,cum.n=8,pt=1)
thetaManually = predict(fit3,d.new)
thetaAIC = predict(AICModel,d.new)
thetaBIC = predict(BICModel,d.new)

sigmaManually = summary(fit3)$sigma
sigmaAIC = summary(AICModel)$sigma
sigmaBIC = summary(BICModel)$sigma

etaManually = exp(thetaManually + (1/2) * sigmaManually^2 )
etaAIC = exp(thetaAIC + (1/2) * sigmaAIC^2 )
etaBIC = exp(thetaBIC + (1/2) * sigmaBIC^2 )

```
From this we got:

* $\hat \eta = 362.10$ for manually selection
* $\hat \eta = 396.10$ for AIC
* $\hat \eta = 366.02$ for BIC


## Problem 2

This script is about splitting the data from nuclear into two different parts, namely training and evaluation set. It starts by selecting a random list by the sample-function. Then two other lists of empty values are defined, RMSE.test1 and RMSE.test2. For this task we use two models, one with all variables and one with no variables. Then we use stepAIC to obtain and predict the models, and then save them in the RMSE.test1 and RMSE.test2. This happens ten times, the stepAIC is done for i = 1,...,10 steps. Note that RMSE.test1 stores test set for log(cost) variable and RMSE.test2 stores for cost variable. At the last, the these are plotted as function of a sequence 0:10. 

```{r}
datadir = "http://www.uio.no/studier/emner/matnat/math/STK2100/data/" 
nuclear = read.table(paste(datadir,"nuclear.dat",sep=""),header=T)
attach(nuclear)

n = nrow(nuclear)
ind = sample(1:n,n/2,replace=FALSE)
RMSE.test1 = rep(NA,11) 
RMSE.test2 = rep(NA,11) 
model_narrow = lm(log(cost) ~ 1, data = nuclear)
model_wide = lm(log(cost) ~ ., data = nuclear)
for(i in 0:10)
{
 
 fit = stepAIC(model_narrow, direction="forward", steps=i,data=nuclear[ind,],
               scope=list(lower=model_narrow, upper=model_wide), k = 0)
 pred = predict(fit,nuclear[-ind,])
 RMSE.test1[i+1] = sqrt(mean((log(nuclear$cost)-pred)^2))
 RMSE.test2[i+1] = sqrt(mean((nuclear$cost-exp(pred))^2))

}
par(mar = c(5, 4, 4, 4) + 0.3)  # Leave space for z axis
plot(0:10,RMSE.test1,xlab="Complexity",ylab="RMSE1",type="l") # first plot
par(new = TRUE)
plot(0:10,RMSE.test2, type = "l", axes = FALSE, bty = "n", xlab = "", ylab = "",col=2)
axis(side=4, at = pretty(range(RMSE.test2)))
mtext("RMSE2", side=4, line=3)

```

The plot of results are shown above. By running this code multiple times the values of the axis in the plot changes, meaning this approach is not very stable. One possible reason why it is like this may have something with randomness to do. Since the variable $ind$ is defined with the sample () function that generates a random list at a given length.

### b) 
In this exercise, we are using 10-folded cross-validation. Again, like previous task, we are using a loop to fit and find the best model. Then we are storing the result for each step into the $RMSE.cv1$ and $RMSE.cv2$. This procedure is more stable than the previous one, but it is also more computationally sensitive and it uses more different models for each prediction. 

```{r}
library(lmvar)
RMSE.cv1 = rep(0,10)
RMSE.cv2 = rep(0,10)
for(i in 0:10)
{
fit = stepAIC(model_narrow, direction="forward", steps=i,k=0,
        scope=list(lower=model_narrow, upper=model_wide),trace=0)
 fit = lm(formula(fit),data=nuclear,x=TRUE,y=TRUE)
 #Note: the k in the command below has a different meaning than k above!!!
 RMSE.cv1[i+1] = cv.lm(fit,k=10)$MSE$mean
 RMSE.cv2[i+1] = cv.lm(fit,k=10,log=TRUE)$MSE$mean
}
par(mar = c(5, 4, 4, 4) + 0.3)  # Leave space for z axis
plot(0:10,RMSE.cv1,xlab="Complexity",ylab="RMSE1",type="l") # first plot
par(new = TRUE)
plot(0:10,RMSE.cv2, type = "l", axes = FALSE, bty = "n", xlab = "", ylab = "",col=2)
axis(side=4, at = pretty(range(RMSE.cv2)))
mtext("RMSE2", side=4, line=3)
```

### c) 
In this exercise, we will modify the previous commands to perform LOOCV. To do this we need a large training set as possible. So the idea is to split the data into 32 sets.

Unfortunately, I could not get further than this. My plan was to implement my idea, but could not pursue it.

## Problem 3

We will in this exercise look at linear regression with quantitative (categorical) explanatory variables. Assume we have data $(c1, y1), ..., (cn, yn)$ where $ci \in {1, ..., K}$. Define for $j = 1, ...., K$

$$ x_{i,j} = 1 {,} c_i = j $$
$$ x_{i,j} = 0 {,} c_i \neq j $$
### a)

We can show that the two models
$$Y_i = \beta_0 + \beta_2 x_{i,2} +...+ \beta_K x_{i,K} + \epsilon_i $$
$$Y_i = \alpha_1 x_{i,1} +...+ \alpha_K x_{i,K} + \epsilon_i $$
are equivalent simply by writing the expressions step by step.

For $c_i = j = 1$ we have
$$Y_i = \beta_0 + \epsilon_i = \alpha_1 + \epsilon_i$$
since every other $x$s are equal to 0 because $c_i \neq j$.

For $c_i = j = 2$ we have
$$Y_i = \beta_0+ \beta_2 + \epsilon_i = \alpha_2 + \epsilon_i $$

For $c_i = j = 3$ we have
$$Y_i = \beta_0+ \beta_3 + \epsilon_i = \alpha_{3} + \epsilon_i $$
We can see a pattern here between $\beta$s and $\alpha$s, and a general relationship can be
$$ \beta_0 + \beta_i = \alpha_i $$
for every $i = 1,...,K$. But not for $\beta_1$, since it is not included in the $Y_i$-function.

### b)
The design matrix can be represented as

$$
X = 
\begin{pmatrix}
x_{11} & x_{12} & \cdots & x_{1k} \\
x_{21} & x_{22} & \cdots & x_{2k} \\
\vdots  & \vdots  & \ddots & \vdots  \\
x_{n1} & x_{n2} & \cdots & x_{nk} 
\end{pmatrix}
$$



Then we have:

$$
X^T X =
\begin{pmatrix}
x_{11} & x_{21} & \cdots & x_{n1}  \\
x_{12} & x_{22} & \cdots & x_{n2} \\
\vdots  & \vdots  & \ddots & \vdots  \\
x_{1k} & x_{2k}  & \cdots & x_{nk} 
\end{pmatrix}
\begin{pmatrix}
x_{11} & x_{12} & \cdots & x_{1k} \\
x_{21} & x_{22} & \cdots & x_{2k} \\
\vdots  & \vdots  & \ddots & \vdots  \\
x_{n1} & x_{n2} & \cdots & x_{nk} 
\end{pmatrix}
$$

Multiplication of matrices is done by multiplying rows of the left matrix with every columns of the right matrix, and end up a new matrix of size (Kxn) containing sum of each row and each columns like

$$
  \sum_{i = 1} ^n x_{m,i} \cdot x_{m,j}
$$

in the new matrix. The $x_{m,i}$ and $x_{m,j}$ can both be equal to $1$ if $c_i = j = i$, and if $i \neq j$ and/or $c_i \neq j$ then the $x_{m,i}$ and $x_{m,j}$ are not equal to $1$. So there we will end up with a diagonal matrix with diagoanl elements $n_j$. In other words:

$$
  \sum_{i = 1} ^n x_{m,i} \cdot x_{m,j} = \bigg \{_{0, else} ^{1, i = j}
$$

We can do the same for

$$
X^T y =
\begin{pmatrix}
x_{11} & x_{21} & \cdots & x_{n1}  \\
x_{12} & x_{22} & \cdots & x_{n2} \\
\vdots  & \vdots  & \ddots & \vdots  \\
x_{1k} & x_{2k}  & \cdots & x_{nk} 
\end{pmatrix}
\begin{pmatrix}
y_1  \\
y_2 \\
\vdots  \\
y_n
\end{pmatrix}
$$

We get almost the same answer as above when we multiply this two, we will end up with a new vector of size (1xn) with elements such as

$$
\sum _ {i =1}^n x_{m,i} y_i
$$

As mentioned above, $x_{m,i}$ will be $1$ and/or $0$ if $c_i = j$ and/or $c_i \neq j$. So for the j-element we get

$$
  \sum _ {i,c_i =j} y_i
$$

We use the formula (from formula collection STK1100/STK1110)

$$ \hat \alpha = (X^T X)^{-1} X^T Y  $$


to find least squares estimates for $\alpha_1, ...,\alpha_K$. We get



$$
\begin{pmatrix}
1/n_1 & 0 & \cdots & 0 \\
0 & 1/n_2 & \cdots & 0 \\
\vdots  & \vdots  & \ddots & \vdots  \\
0 & 0 & \cdots & 1/n_k 
\end{pmatrix}
\begin{pmatrix}
\sum _ {i,c_i =1} y_i \\
\sum _ {i,c_i =2} y_i \\
\vdots  \\
\sum _ {i,c_i =k} y_i
\end{pmatrix}
$$

$$
=
\begin{pmatrix}
\frac{\sum _ {i,c_i =1}y_i}{n_1} \\
\frac{\sum _ {i,c_i =2}y_i}{n_2}\\
\vdots \\
\frac{\sum _ {i,c_i =K}y_i}{n_K}
\end{pmatrix}
$$
So, the j-th element of the least square estimate is

$$
  \hat \alpha = \frac{1}{n_j} \sum _ {i,c_i =j}y_i
$$



### c)
From task a), we had the following relationship between $\alpha$ and $\beta$:
$$\alpha_i = \beta_0 + \beta_i$$
which was true for every $i = 1,2,...,k$ but $\beta_0$ is always equal to 0. By rearranging the expression we get the least square estimate for
$$ \hat \beta_i =\alpha_i- \beta_0= \hat \alpha_i  - \hat \alpha_1$$

### d)

We will use same procedure as in task 1), finding relationship through writing out the expression. So for $c_i = j =1$ we get
$$ Y_i = \gamma_0 + \gamma_1 + \epsilon_i $$
Similarly we get for $c_i = 2$ and $c_i = 3$
$$ Y_i = \gamma_0 + \gamma_2 + \epsilon_i $$
$$ Y_i = \gamma_0 + \gamma_3 + \epsilon_i $$
We can see a pattern here, and it can be generalized by
$$ \alpha_i = \gamma_0 + \gamma_i$$
By taking the sum over all explanatory variables, we get:
$$ \sum_{i = 1} ^K \alpha_i = \sum_{i = 1} ^K \gamma_0 + \sum_{i = 1} ^K \gamma_i$$
$$ =K \gamma_0 + \sum_{i = 1} ^K \gamma_i$$
From the exercise, we had that $\sum_{i = 1} ^K \gamma_i = 0$, and we get an expression for
$$\gamma_0 = \frac{1}{K}  \sum_{i = 1} ^K \alpha_i = \overline{\alpha}$$
The $\gamma_j$ must have the values of
$$ \gamma_j = \alpha_j - \gamma_0 =  \alpha_j -  \overline{\alpha}$$
in order for this model to be equivalent to the previous models. To obtain the same results for $\beta$s, one need to insert $\alpha_j = \beta_j + \beta_0$ in the expression above
$$\gamma_j = \beta_j + \beta_0 -  \overline{\alpha}  = \beta_j + \beta_0 -\frac{1}{K}  \sum_{i = 1} ^K \beta_j + \beta_0 $$
$$ =\beta_j + \beta_0 - \frac{K}{K} \beta_0 -\sum_{i = 1} ^K \beta_j
 = \beta_j - \frac{1}{K} \sum_{i = 1} ^K \beta_j= \beta_j -  \overline{\beta}$$

### e)
Using the code provided in the exercise:
```{r}
datadir = "http://www.uio.no/studier/emner/matnat/math/STK2100/data/"
Fe <- read.table(paste(datadir,"fe.txt",sep=""),
                 header=T,sep=",")
fit <- lm(Fe~form,data=Fe)
summary(fit)

```
Here we can see that R is not distinguish between the categorical variables, but we must use the other code

```{r}
Fe$form <- as.factor(Fe$form)
fit1 <- lm(Fe~form-1,data=Fe)
summary(fit1)
```
Now, R has distinguished between the categorical factors and we see four variable in the regression above. Because of that we get incredibly better result. The predicted model look like the $\alpha$-model since it has got no intercept.

### f)
```{r}
options()$contrasts
options(contrasts=c("contr.treatment","contr.treatment"))
fit2 <- lm(Fe~form,data=Fe)
summary(fit2)
```
This model looks like the $\beta$-model since it has got an intercept and no "$\beta_1$" ("form1") variable, but it has $\beta_2$, $\beta_3$ and $\beta_4$.


```{r}
options(contrasts=c("contr.sum","contr.sum"))
options()$contrasts
fit3 <- lm(Fe~form,data=Fe)
summary(fit3)
-sum(fit3$coefficients[-1])

```
This model looks like the $\gamma$-model since it has got both the intercept and the other explanatory variables.

All of this is summarized in the table below

$$| -             | Intercept | form1  | form2  | form3 | form4 |$$
$$|\alpha-model | -         |26.080  |24.690  |29.950 |33.840 |$$
$$|\beta-model  |26.080     | -      | -1.390 |3.870  |  7.760|$$
$$|\gamma-model |28.6400    |-2.5600 |-3.9500 |1.3100 |5.2    |$$


### g)
In order to test the difference between different types of iron, we can do a test with null hypothesis as every variables are equal to zero against alternative hypothesis such as they are not equal to zero. We can write it as
$$H_0: \beta_j = 0 $$
against $H_a:$ at least one $\beta_j = 0$. We perform this test for all three models.

We can use the fitted models to perform this tests. Generally, the smaller p-value a fitted model have the more evidence there is in the sample data against the null hypothesis. By observing the p-value for all three models, we can see that they are extremely small and therefore we can reject the null hypothesis. We can conclude with that there is a certain form of difference between the 4 iron types.

### h)
A possible simplification of the model could have been to add together the variables that are almost the same. From the output of model we can see that form1 and form2 have almost equal t-value, and can therefore be merged since they have equal properties and it simplifies the model.

